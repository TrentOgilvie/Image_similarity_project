{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXWdRDONNn6d0jIwOXPF4H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TrentOgilvie/Image_similarity_project/blob/main/Image_similarity_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ8e0gKBHEmX"
      },
      "outputs": [],
      "source": [
        "# SETUP SECTION: Install required libraries\n",
        "# Install dependencies for image processing and deep learning\n",
        "!pip install tensorflow matplotlib opencv-python-headless\n",
        "\n",
        "# Import all necessary libraries\n",
        "import numpy as np              # For numerical operations on arrays\n",
        "import cv2                      # OpenCV for image processing\n",
        "from tensorflow.keras.models import Model  # Keras API for building neural networks\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Lambda, BatchNormalization, concatenate\n",
        "import tensorflow.keras.backend as K      # Keras backend functions\n",
        "from tensorflow.keras.applications import VGG16  # Pre-trained model for transfer learning\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "from google.colab import files   # For file uploads in Google Colab\n",
        "import os\n",
        "import gc  # Garbage collector to free up memory\n",
        "\n",
        "# IMAGE PREPROCESSING FUNCTION\n",
        "def preprocess_image(image_path, size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Loads, resizes, and normalizes an image for the neural network.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        size: Target dimensions for the image\n",
        "\n",
        "    Returns:\n",
        "        Processed image as a normalized numpy array\n",
        "    \"\"\"\n",
        "    # Load the image from disk\n",
        "    image = cv2.imread(image_path)\n",
        "    # Convert from BGR (OpenCV default) to RGB color format\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    # Resize to our target dimensions\n",
        "    image = cv2.resize(image, size)\n",
        "    # Normalize pixel values to range [0,1]\n",
        "    image = image / 255.0\n",
        "    return image\n",
        "\n",
        "# TRAINING DATA GENERATION\n",
        "def generate_training_data(num_pairs=1000, size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Creates synthetic image pairs for training the Siamese network.\n",
        "    Generates both similar and dissimilar pairs with labels.\n",
        "\n",
        "    Args:\n",
        "        num_pairs: Number of image pairs to generate\n",
        "        size: Size of generated images\n",
        "\n",
        "    Returns:\n",
        "        Three arrays: First images, second images, and similarity scores\n",
        "    \"\"\"\n",
        "    # Process in batches to reduce memory usage\n",
        "    batch_size = 200\n",
        "    X1, X2, Y = [], [], []\n",
        "\n",
        "    for batch in range(0, num_pairs, batch_size):\n",
        "        # Calculate actual batch size (might be smaller for last batch)\n",
        "        curr_batch_size = min(batch_size, num_pairs - batch)\n",
        "\n",
        "        # Create temporary batch containers\n",
        "        X1_batch, X2_batch, Y_batch = [], [], []\n",
        "\n",
        "        for i in range(curr_batch_size):\n",
        "            # Create random base image\n",
        "            base_img = np.random.rand(size[0], size[1], 3)\n",
        "\n",
        "            if i % 2 == 0:\n",
        "                # For even indices: Create SIMILAR pairs\n",
        "                # Add small random noise to create a slightly modified version\n",
        "                noise = np.random.normal(0, 0.02, size=(size[0], size[1], 3))\n",
        "                modified_img = np.clip(base_img + noise, 0, 1)  # Keep values in [0,1]\n",
        "                # Assign high similarity score (0.8-1.0)\n",
        "                similarity = np.random.uniform(0.8, 1.0)\n",
        "            else:\n",
        "                # For odd indices: Create DIFFERENT pairs\n",
        "                # Create completely different random image\n",
        "                modified_img = np.random.rand(size[0], size[1], 3)\n",
        "                # Assign low similarity score (0.0-0.2)\n",
        "                similarity = np.random.uniform(0.0, 0.2)\n",
        "\n",
        "            # Add to batch arrays\n",
        "            X1_batch.append(base_img)\n",
        "            X2_batch.append(modified_img)\n",
        "            Y_batch.append(similarity)\n",
        "\n",
        "        # Add batch to overall arrays\n",
        "        X1.extend(X1_batch)\n",
        "        X2.extend(X2_batch)\n",
        "        Y.extend(Y_batch)\n",
        "\n",
        "        # Clean up to free memory\n",
        "        del X1_batch, X2_batch, Y_batch\n",
        "        gc.collect()\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    return np.array(X1), np.array(X2), np.array(Y)\n",
        "\n",
        "# FEATURE EXTRACTION MODEL\n",
        "def create_feature_extraction_model(input_shape=(256, 256, 3)):\n",
        "    \"\"\"\n",
        "    Creates a model to extract features from different VGG16 layers.\n",
        "    Each layer captures different levels of image characteristics.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Dimensions of input images\n",
        "\n",
        "    Returns:\n",
        "        Keras model that outputs features from multiple layers\n",
        "    \"\"\"\n",
        "    # Load pre-trained VGG16 model (without the classification layers)\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Select layers from different depths of the network:\n",
        "    layer_outputs = [\n",
        "        base_model.get_layer('block1_conv2').output,  # Low-level features (edges, textures)\n",
        "        base_model.get_layer('block3_conv3').output,  # Mid-level features (patterns)\n",
        "        base_model.get_layer('block5_conv3').output   # High-level features (semantic content)\n",
        "    ]\n",
        "\n",
        "    # Create model with multiple outputs\n",
        "    return Model(inputs=base_model.input, outputs=layer_outputs)\n",
        "\n",
        "# SIAMESE NETWORK ARCHITECTURE\n",
        "def siamese_network(input_shape=(256, 256, 3)):\n",
        "    \"\"\"\n",
        "    Builds a Siamese neural network for image comparison.\n",
        "    Uses the same base network to process both images, then compares results.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Dimensions of input images\n",
        "\n",
        "    Returns:\n",
        "        The complete Siamese model and the base network\n",
        "    \"\"\"\n",
        "    def create_base_network(input_shape):\n",
        "        \"\"\"\n",
        "        Internal function to create the shared feature extraction network.\n",
        "        Uses VGG16 with additional layers for fine-tuning.\n",
        "        \"\"\"\n",
        "        # Load pre-trained VGG16\n",
        "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "        # Make the last 12 layers trainable (fine-tuning)\n",
        "        for layer in base_model.layers[-12:]:\n",
        "            layer.trainable = True\n",
        "\n",
        "        # Add custom layers after VGG16 for better feature extraction\n",
        "        x = base_model.output\n",
        "        x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)  # Additional convolution\n",
        "        x = BatchNormalization()(x)  # Normalize activations for stable training\n",
        "        x = MaxPooling2D((2, 2))(x)  # Reduce dimensions\n",
        "        x = Flatten()(x)  # Convert to 1D vector\n",
        "        x = Dense(1024, activation='relu')(x)  # Fully connected layer\n",
        "        x = BatchNormalization()(x)  # Another normalization\n",
        "        x = Dense(512, activation='relu')(x)  # Final embedding layer\n",
        "        return Model(base_model.input, x)\n",
        "\n",
        "    # Create the shared base network\n",
        "    base_network = create_base_network(input_shape)\n",
        "\n",
        "    # Define two inputs (one for each image)\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "\n",
        "    # Process both inputs through the SAME network (parameter sharing)\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    def cosine_similarity(vects):\n",
        "        \"\"\"\n",
        "        Computes cosine similarity between feature vectors.\n",
        "        1 = identical, 0 = orthogonal, -1 = opposite\n",
        "        \"\"\"\n",
        "        x, y = vects\n",
        "        # Normalize vectors to unit length\n",
        "        x = K.l2_normalize(x, axis=-1)\n",
        "        y = K.l2_normalize(y, axis=-1)\n",
        "        # Calculate dot product (similarity)\n",
        "        return K.sum(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "    # Compute similarity between the two processed inputs\n",
        "    similarity = Lambda(cosine_similarity)([processed_a, processed_b])\n",
        "\n",
        "    # Build complete model\n",
        "    model = Model([input_a, input_b], similarity)\n",
        "    # Use mean squared error to compare predicted similarity with actual similarity\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
        "    return model, base_network\n",
        "\n",
        "# DIFFERENCE MAP GENERATION\n",
        "def generate_feature_difference_map(image1, image2, feature_model, size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Creates a detailed difference map highlighting where images differ.\n",
        "    Uses features from multiple network layers for comprehensive comparison.\n",
        "\n",
        "    Args:\n",
        "        image1, image2: The two images to compare\n",
        "        feature_model: Model to extract features\n",
        "        size: Output dimensions\n",
        "\n",
        "    Returns:\n",
        "        RGB difference map highlighting various types of differences\n",
        "    \"\"\"\n",
        "    # Extract features from both images\n",
        "    features1 = feature_model.predict(np.expand_dims(image1, axis=0))\n",
        "    features2 = feature_model.predict(np.expand_dims(image2, axis=0))\n",
        "\n",
        "    # Initialize blank difference map\n",
        "    diff_map = np.zeros(size + (3,))\n",
        "\n",
        "    # Process each feature layer (low, mid, high level)\n",
        "    for i, (feat1, feat2) in enumerate(zip(features1, features2)):\n",
        "        # Calculate absolute difference between feature maps\n",
        "        feat_diff = np.abs(feat1 - feat2)\n",
        "\n",
        "        # Average across channels to get a single difference map\n",
        "        feat_diff_aggregated = np.mean(feat_diff, axis=-1)[0]\n",
        "\n",
        "        # Resize to original image size\n",
        "        feat_diff_resized = cv2.resize(feat_diff_aggregated, size)\n",
        "\n",
        "        # Normalize values to [0,1] range\n",
        "        feat_diff_resized = (feat_diff_resized - feat_diff_resized.min()) / \\\n",
        "                           (feat_diff_resized.max() - feat_diff_resized.min() + 1e-8)\n",
        "\n",
        "        # Weight different feature levels differently\n",
        "        # Higher level features get more weight (considered more important)\n",
        "        weight = [0.2, 0.3, 0.5][i]\n",
        "\n",
        "        # Add to composite difference map\n",
        "        # Low-level differences go to red channel, mid to green, high to blue\n",
        "        diff_map[:, :, i] += feat_diff_resized * weight\n",
        "\n",
        "    # Normalize each color channel\n",
        "    for c in range(3):\n",
        "        if diff_map[:, :, c].max() > diff_map[:, :, c].min():\n",
        "            diff_map[:, :, c] = (diff_map[:, :, c] - diff_map[:, :, c].min()) / \\\n",
        "                               (diff_map[:, :, c].max() - diff_map[:, :, c].min())\n",
        "\n",
        "    return diff_map\n",
        "\n",
        "# MAIN EXECUTION FLOW\n",
        "# Upload images\n",
        "print(\"Please upload the two images (before and after).\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Verify and save uploads\n",
        "image_paths = list(uploaded_files.keys())\n",
        "if len(image_paths) != 2:\n",
        "    raise ValueError(\"Please upload exactly two images: one for 'before' and one for 'after'.\")\n",
        "\n",
        "# Process uploaded images\n",
        "image1 = preprocess_image(image_paths[0])\n",
        "image2 = preprocess_image(image_paths[1])\n",
        "\n",
        "# Create neural network models\n",
        "siamese, base_network = siamese_network()\n",
        "feature_model = create_feature_extraction_model()\n",
        "\n",
        "# Generate training data\n",
        "print(\"\\nGenerating training data...\")\n",
        "X1_train, X2_train, Y_train = generate_training_data(num_pairs=1000)\n",
        "\n",
        "# Train the Siamese network\n",
        "print(\"\\nTraining Siamese Network...\")\n",
        "siamese.fit(\n",
        "    [X1_train, X2_train],\n",
        "    Y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Free memory\n",
        "del X1_train, X2_train, Y_train\n",
        "gc.collect()\n",
        "\n",
        "# Calculate similarity between the two uploaded images\n",
        "similarity_score = siamese.predict([\n",
        "    np.expand_dims(image1, axis=0),\n",
        "    np.expand_dims(image2, axis=0)\n",
        "])[0][0]\n",
        "\n",
        "# DIFFERENCE VISUALIZATION METHODS\n",
        "\n",
        "# Method 1: Simple pixel-by-pixel difference\n",
        "pixel_diff = np.abs(image1 - image2)\n",
        "pixel_diff = (pixel_diff - pixel_diff.min()) / (pixel_diff.max() - pixel_diff.min() + 1e-8)\n",
        "\n",
        "# Method 2: Feature-based difference map using neural network\n",
        "print(\"\\nGenerating detailed difference map based on Siamese network features...\")\n",
        "feature_diff_map = generate_feature_difference_map(image1, image2, feature_model)\n",
        "\n",
        "# Create a heatmap by averaging all channels\n",
        "heatmap = np.mean(feature_diff_map, axis=-1)\n",
        "heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
        "\n",
        "# VISUALIZATION\n",
        "# Display results in a 2x3 grid of images\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Original images\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.title(\"Image Before\")\n",
        "plt.imshow(image1)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.title(\"Image After\")\n",
        "plt.imshow(image2)\n",
        "plt.axis('off')\n",
        "\n",
        "# Simple pixel difference\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.title(f\"Simple Pixel Difference\\nSimilarity Score: {similarity_score:.3f}\")\n",
        "plt.imshow(np.mean(pixel_diff, axis=-1), cmap='hot')\n",
        "plt.colorbar(label='Difference Magnitude')\n",
        "plt.axis('off')\n",
        "\n",
        "# Feature-based difference as RGB image\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.title(\"Feature Difference Map (RGB)\")\n",
        "plt.imshow(feature_diff_map)\n",
        "plt.axis('off')\n",
        "\n",
        "# Feature-based difference as heatmap\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.title(\"Feature Difference Heatmap\")\n",
        "plt.imshow(heatmap, cmap='inferno')\n",
        "plt.colorbar(label='Difference Magnitude')\n",
        "plt.axis('off')\n",
        "\n",
        "# Overlay visualization\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.title(\"Overlay on Image After\")\n",
        "overlay = image2.copy()\n",
        "# Create mask where differences are significant\n",
        "mask = heatmap > 0.5\n",
        "# Highlight differences in red\n",
        "overlay[mask] = np.array([1.0, 0.0, 0.0])  # Red highlighting\n",
        "plt.imshow(overlay)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print interpretation guidelines\n",
        "print(f\"\\nSimilarity Score: {similarity_score:.3f}\")\n",
        "print(\"Interpretation:\")\n",
        "print(\"0.0-0.2: Very Different\")\n",
        "print(\"0.2-0.8: Moderately Similar\")\n",
        "print(\"0.8-1.0: Very Similar\")\n",
        "print(\"\\nThe detailed difference map uses Siamese network feature representations to highlight:\")\n",
        "print(\"- Red channel: Low-level differences (textures, edges)\")\n",
        "print(\"- Green channel: Mid-level differences (patterns, parts)\")\n",
        "print(\"- Blue channel: High-level differences (semantic features)\")\n",
        "print(\"\\nBrighter areas indicate larger differences between the images.\")"
      ]
    }
  ]
}